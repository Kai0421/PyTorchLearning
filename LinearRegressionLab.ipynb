{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWYIEloINQx8u/dwPiULSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kai0421/PyTorchLearning/blob/main/LinearRegressionLab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Squared Error Function\n",
        "Using the below equation to find the cost of function J\n",
        "\\begin{align}\n",
        "  f_{(w,b)} &= w x + b \\\\\n",
        "  \\hat{y} &= w x + b \\\\\n",
        "  J_{(w,b)} &= {\\frac 1 {2m}} \\sum_{n=1}^m (\\hat{y}^{(i)} - y )^2 \\\\\n",
        "  J_{(w,b)} &= {\\frac 1 {2m}} \\sum_{n=1}^m (f_{(w,b)}(x^{(i)}) - y )^2 \\\\\n",
        "  J_{(w,b)} &= {\\frac 1 {2m}} \\sum_{n=1}^m ( (wx^{(i)} + b ) - y )^2\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "rglSjwb_RJ4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JXkPwprcb7b3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = np.array([1, 2, 3, 4])\n",
        "y_train = np.array([100, 200, 300, 400])\n",
        "\n",
        "def compute_cost(x, y, w, b):\n",
        "\n",
        "  m = x.shape[0]\n",
        "  cost_sum = 0\n",
        "  for i in range(m):\n",
        "    f_w_b = (w * x[i]) + b\n",
        "    cost = (f_w_b - y[i])**2\n",
        "    cost_sum = cost_sum + cost\n",
        "\n",
        "  square_error = (1/2*m) * cost_sum\n",
        "\n",
        "  return square_error;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent \n",
        "Gradient descent finds the mins cost of function J:\n",
        "\n",
        "\\begin{align}\n",
        "  W &= w - α ⋅ \\boxed{{\\frac ∂ {∂w}} J(w,b)} \\\\\n",
        "  W &= w - α ⋅ \\boxed{{\\frac 1 m} \\sum_{n=1}^m (f_{(w,b)}(x^{(i)}) - y^{(i)} )x^{(i)} } \\\\\n",
        "  B &= b - α ⋅ \\boxed{{\\frac ∂ {∂b}} J(w,b)} \\\\\n",
        "  B &= b - α ⋅ \\boxed{{\\frac 1 m} \\sum_{n=1}^m (f_{(w,b)}(x^{(i)}) - y^{(i)} )}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "Zn7ZHk9EzRHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_gradient(x, y, w, b):\n",
        "  m = x.shape[0]\n",
        "\n",
        "  w_cost_calc = 0\n",
        "  b_cost_calc = 0\n",
        "\n",
        "  for i in range(m):\n",
        "    # W calculation\n",
        "    f_w_b = (x[i] * w) + b\n",
        "    cost = f_w_b - y[i]\n",
        "    w_cost_calc = w_cost_calc + (cost * x[i])\n",
        "    b_cost_calc = b_cost_calc + cost\n",
        "\n",
        "  dj_dw = w_cost_calc/m\n",
        "  dj_db = b_cost_calc/m\n",
        "  \n",
        "  return dj_dw, dj_db; \n",
        "\n",
        "def gradient_descent(x, y, w_input, b_input, iteration, alpha, cost_function, gradient_function):\n",
        "\n",
        "  J_history=[]\n",
        "  p_history=[]\n",
        "\n",
        "  w = w_input\n",
        "  b = b_input\n",
        "\n",
        "  for i in range(iteration):\n",
        "    dj_dw, dj_db = gradient_function(x, y, w, b)\n",
        "\n",
        "    w = w - alpha * dj_dw\n",
        "    b = b - alpha * dj_db\n",
        "\n",
        "    # Save cost J at each iteration\n",
        "    if i<100000:      # prevent resource exhaustion \n",
        "        J_history.append( cost_function(x, y, w , b))\n",
        "        p_history.append([w,b])\n",
        "    # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "    if i% math.ceil(iteration/10) == 0:\n",
        "        print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
        "              f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
        "              f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
        " \n",
        "  return w, b, J_history, p_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "vZJffLBICbE-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize value\n",
        "w_init = 0\n",
        "b_init = 0\n",
        "\n",
        "# Gradient Descent initialization\n",
        "iteration = 10000\n",
        "tmp_alpha = 1.0e-2\n",
        "\n",
        "w_final, b_final, j_history, p_history = gradient_descent( x_train, y_train, w_init, b_init, iteration, tmp_alpha, compute_cost, compute_gradient)\n",
        "\n",
        "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKMsAI1TH4Zp",
        "outputId": "ca4ba18e-fe93-4e4f-d317-e5d2eae0d3fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost 5.04e+05  dj_dw: -7.500e+02, dj_db: -2.500e+02   w:  7.500e+00, b: 2.50000e+00\n",
            "Iteration 1000: Cost 6.19e+01  dj_dw: -3.470e-01, dj_db:  1.020e+00   w:  9.769e+01, b: 6.80516e+00\n",
            "Iteration 2000: Cost 3.09e+00  dj_dw: -7.758e-02, dj_db:  2.281e-01   w:  9.948e+01, b: 1.52137e+00\n",
            "Iteration 3000: Cost 1.55e-01  dj_dw: -1.734e-02, dj_db:  5.099e-02   w:  9.988e+01, b: 3.40117e-01\n",
            "Iteration 4000: Cost 7.72e-03  dj_dw: -3.877e-03, dj_db:  1.140e-02   w:  9.997e+01, b: 7.60368e-02\n",
            "Iteration 5000: Cost 3.86e-04  dj_dw: -8.668e-04, dj_db:  2.548e-03   w:  9.999e+01, b: 1.69988e-02\n",
            "Iteration 6000: Cost 1.93e-05  dj_dw: -1.938e-04, dj_db:  5.697e-04   w:  1.000e+02, b: 3.80027e-03\n",
            "Iteration 7000: Cost 9.64e-07  dj_dw: -4.332e-05, dj_db:  1.274e-04   w:  1.000e+02, b: 8.49591e-04\n",
            "Iteration 8000: Cost 4.82e-08  dj_dw: -9.685e-06, dj_db:  2.847e-05   w:  1.000e+02, b: 1.89935e-04\n",
            "Iteration 9000: Cost 2.41e-09  dj_dw: -2.165e-06, dj_db:  6.366e-06   w:  1.000e+02, b: 4.24620e-05\n",
            "(w,b) found by gradient descent: (100.0000,  0.0000)\n"
          ]
        }
      ]
    }
  ]
}